{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Spark Advanced Topics Working Group Documentation Welcome to the Spark Advanced Topics working group documentation. This documentation is in the early stages. We have been working on a flowchart to help you solve your current problem. The documentation is collected under \"details\" (see above). Other resources Some other resources that may be useful include High Performance Spark by Holden Karau and Rachel Warren (note: some bias as a co-author), as well as the excellent on-line The Internals of Apache Spark 3.2.0 and The Internals of Spark SQL by Jacek Laskowski.","title":"Spark Advanced Topics Working Group Documentation"},{"location":"#spark-advanced-topics-working-group-documentation","text":"Welcome to the Spark Advanced Topics working group documentation. This documentation is in the early stages. We have been working on a flowchart to help you solve your current problem. The documentation is collected under \"details\" (see above).","title":"Spark Advanced Topics Working Group Documentation"},{"location":"#other-resources","text":"Some other resources that may be useful include High Performance Spark by Holden Karau and Rachel Warren (note: some bias as a co-author), as well as the excellent on-line The Internals of Apache Spark 3.2.0 and The Internals of Spark SQL by Jacek Laskowski.","title":"Other resources"},{"location":"details/bad_partioning/","text":"Bad partioning There are three main different types and causes of bad partioning in Spark. Partioning is often the limitation of parallelism for most Spark jobs. The most common (and most difficult to fix) bad partioning in Spark is that of skewed partioning. With key-skew the problem is not the number of partions, but that the data is not evenly distributed amongst the partions. The most frequent cause of skewed partioning is that of \"key-skew.\" . This happens frequently since humans and machines both tend to cluster resulting in skew (e.g. NYC and null ). The other type of skewed partioning comes from \"input partioned\" data which is not evenly partioned. With input partioned data, the RDD or Dataframe doesn't have a particular partioner it just matches however the data is stored on disk. Uneven input partioned data can be fixed with an explicit repartion/shuffle. Insufficent partioning is similar to input skewed partioning, except instead of skew there just are not enough partions. Similarily you the number of partions (e.g. repartion(5000) or change spark.sql.shuffle.partitions ).","title":"Bad partioning"},{"location":"details/bad_partioning/#bad-partioning","text":"There are three main different types and causes of bad partioning in Spark. Partioning is often the limitation of parallelism for most Spark jobs. The most common (and most difficult to fix) bad partioning in Spark is that of skewed partioning. With key-skew the problem is not the number of partions, but that the data is not evenly distributed amongst the partions. The most frequent cause of skewed partioning is that of \"key-skew.\" . This happens frequently since humans and machines both tend to cluster resulting in skew (e.g. NYC and null ). The other type of skewed partioning comes from \"input partioned\" data which is not evenly partioned. With input partioned data, the RDD or Dataframe doesn't have a particular partioner it just matches however the data is stored on disk. Uneven input partioned data can be fixed with an explicit repartion/shuffle. Insufficent partioning is similar to input skewed partioning, except instead of skew there just are not enough partions. Similarily you the number of partions (e.g. repartion(5000) or change spark.sql.shuffle.partitions ).","title":"Bad partioning"},{"location":"details/big-broadcast-join/","text":"Too big broadcast joins Beware that broadcast joins put unnecessary pressure on the driver. Before the tables are broadcasted to all the executors, the data is brought back to the driver and then broadcasted to executors. So you might run into driver OOMs. Broadcast smaller tables but this is usually recommended for < 10 Mb tables. Although that is mostly the default, we can comfortably broadcast much larger datasets as long as they fit in the executor and driver memories. Remember if there are multiple broadcast joins in the same stage, you need to have enough room for all those datasets in memory. You can configure the broadcast threshold using spark.sql.autoBroadcastJoinThreshold or increase the driver memory by setting spark.driver.memory to a higher value Make sure that you need more memory on your driver than the sum of all your broadcasted data in any stage plus all the other overheads that the driver deals with!","title":"Too big broadcast joins"},{"location":"details/big-broadcast-join/#too-big-broadcast-joins","text":"Beware that broadcast joins put unnecessary pressure on the driver. Before the tables are broadcasted to all the executors, the data is brought back to the driver and then broadcasted to executors. So you might run into driver OOMs. Broadcast smaller tables but this is usually recommended for < 10 Mb tables. Although that is mostly the default, we can comfortably broadcast much larger datasets as long as they fit in the executor and driver memories. Remember if there are multiple broadcast joins in the same stage, you need to have enough room for all those datasets in memory. You can configure the broadcast threshold using spark.sql.autoBroadcastJoinThreshold or increase the driver memory by setting spark.driver.memory to a higher value Make sure that you need more memory on your driver than the sum of all your broadcasted data in any stage plus all the other overheads that the driver deals with!","title":"Too big broadcast joins"},{"location":"details/broadcast-with-disable/","text":"Tables getting broadcasted even when broadcast is disabled You expect the broadcast to stop after you disable the broadcast threshold, by setting spark.sql.autoBroadcastJoinThreshold to -1, but Spark tries to broadcast the bigger table and fails with a broadcast error. And you observe that the query plan has BroadcastNestedLoopJoin in the physical plan. Check for sub queries in your code using NOT IN Example : select * from TableA where id not in (select id from TableB) This typically results in a forced BroadcastNestedLoopJoin even when the broadcast setting is disabled. If the data being processed is large enough, this results in broadcast errors when Spark attempts to broadcast the table Rewrite query using not exists or a regular LEFT JOIN instead of not in Example: select * from TableA where not exists (select 1 from TableB where TableA.id = TableB.id) The query will use SortMergeJoin and will resolve any Driver memory errors because of forced broadcasts Relevant links External Resource","title":"Tables getting broadcasted even when broadcast is disabled"},{"location":"details/broadcast-with-disable/#tables-getting-broadcasted-even-when-broadcast-is-disabled","text":"You expect the broadcast to stop after you disable the broadcast threshold, by setting spark.sql.autoBroadcastJoinThreshold to -1, but Spark tries to broadcast the bigger table and fails with a broadcast error. And you observe that the query plan has BroadcastNestedLoopJoin in the physical plan. Check for sub queries in your code using NOT IN Example : select * from TableA where id not in (select id from TableB) This typically results in a forced BroadcastNestedLoopJoin even when the broadcast setting is disabled. If the data being processed is large enough, this results in broadcast errors when Spark attempts to broadcast the table Rewrite query using not exists or a regular LEFT JOIN instead of not in Example: select * from TableA where not exists (select 1 from TableB where TableA.id = TableB.id) The query will use SortMergeJoin and will resolve any Driver memory errors because of forced broadcasts","title":"Tables getting broadcasted even when broadcast is disabled"},{"location":"details/broadcast-with-disable/#relevant-links","text":"External Resource","title":"Relevant links"},{"location":"details/collect/","text":"Bringing too much data back to the driver (collect and friends) A common anti-pattern in Apache Spark is using collect() and then processing records on the driver. There are a few different reasons why folks tend to do this and we can work through some alternatives: Label items in ascending order ZipWithIndex Index items in order Compute the size of each partion use this to assign indexes. In order processing Compute a partion at a time (this is annoying to do, sorry). Writing out to a format not supported by Spark Use foreachPartition or implement your own DataSink. Need to aggregate everything into a single record Call reduce or treeReduce Sometimes you do really need to bring the data back to the driver for some reason (e.g., updating model weights). In those cases, especially if you process the data sequentially, you can limit the amount of data coming back to the driver at one time. toLocalIterator gives you back an iterator which will only need to fetch a partion at a time (although in Python this may be pipeline for efficency). By default toLocalIterator will launch a Spark job for each partition, so if you know you will eventually need all of the data it makes sense to do a persist + a count (async or otherwise) so you don't block as long between partions. This doesn't mean every call to collect() is bad, if the amount of data being returned is under ~1gb it's probably OK although it will limit parallelism.","title":"Bringing too much data back to the driver (collect and friends)"},{"location":"details/collect/#bringing-too-much-data-back-to-the-driver-collect-and-friends","text":"A common anti-pattern in Apache Spark is using collect() and then processing records on the driver. There are a few different reasons why folks tend to do this and we can work through some alternatives: Label items in ascending order ZipWithIndex Index items in order Compute the size of each partion use this to assign indexes. In order processing Compute a partion at a time (this is annoying to do, sorry). Writing out to a format not supported by Spark Use foreachPartition or implement your own DataSink. Need to aggregate everything into a single record Call reduce or treeReduce Sometimes you do really need to bring the data back to the driver for some reason (e.g., updating model weights). In those cases, especially if you process the data sequentially, you can limit the amount of data coming back to the driver at one time. toLocalIterator gives you back an iterator which will only need to fetch a partion at a time (although in Python this may be pipeline for efficency). By default toLocalIterator will launch a Spark job for each partition, so if you know you will eventually need all of the data it makes sense to do a persist + a count (async or otherwise) so you don't block as long between partions. This doesn't mean every call to collect() is bad, if the amount of data being returned is under ~1gb it's probably OK although it will limit parallelism.","title":"Bringing too much data back to the driver (collect and friends)"},{"location":"details/container-oom/","text":"Container OOMs Container OOMs can be difficult to debug as the container running the problematic code is killed, and sometimes not all of the log information is available. Non-JVM language users (such as Python) are most likely to encounter issues with container OOMs. This is because the JVM is generally configured to not use more memory than the container it is running in. Everything which isn't inside the JVM is considered \"overhead\", so Tensorflow, Python, bash, etc. A first step with a container OOM is often increasing spark.executor.memoryOverhead and spark.driver.memoryOverhead to leave more memory for non-Java processes. Python users can set spark.executor.pyspark.memory to limit the Python VM to a certain amount of memory. This amount of memory is then added to the overhead. Python users performing aggregations in Python should also check out the PyUDFOOM page .","title":"Container OOMs"},{"location":"details/container-oom/#container-ooms","text":"Container OOMs can be difficult to debug as the container running the problematic code is killed, and sometimes not all of the log information is available. Non-JVM language users (such as Python) are most likely to encounter issues with container OOMs. This is because the JVM is generally configured to not use more memory than the container it is running in. Everything which isn't inside the JVM is considered \"overhead\", so Tensorflow, Python, bash, etc. A first step with a container OOM is often increasing spark.executor.memoryOverhead and spark.driver.memoryOverhead to leave more memory for non-Java processes. Python users can set spark.executor.pyspark.memory to limit the Python VM to a certain amount of memory. This amount of memory is then added to the overhead. Python users performing aggregations in Python should also check out the PyUDFOOM page .","title":"Container OOMs"},{"location":"details/even_partioning_still_slow/","text":"Even partioning, but still slow Sometimes you will have a stage with even partioning where it is still slow. To see if a stage if evenly partioned take a look the stage page and look at the distribution of data sizes and durations of the completed tasks. If the max task duration is still substantailly shorter than the stages overall duration, this is often a sign of an insufficient number of executors. Spark can run (at most) spark.executor.cores * spark.dynamicAllocation.maxExecutors tasks in parallel (and in practice this will be lower since some tasks will be speculatively executed and some executors will fail). Try increasing the maxExecutors and seeing if your job speeds up. If the data is evenly partionined but the max task duration is longer than desired for the stage, increasing the number of executors will not help and you'll need to re-partition the data. See bad partioning .","title":"Even partioning, but still slow"},{"location":"details/even_partioning_still_slow/#even-partioning-but-still-slow","text":"Sometimes you will have a stage with even partioning where it is still slow. To see if a stage if evenly partioned take a look the stage page and look at the distribution of data sizes and durations of the completed tasks. If the max task duration is still substantailly shorter than the stages overall duration, this is often a sign of an insufficient number of executors. Spark can run (at most) spark.executor.cores * spark.dynamicAllocation.maxExecutors tasks in parallel (and in practice this will be lower since some tasks will be speculatively executed and some executors will fail). Try increasing the maxExecutors and seeing if your job speeds up. If the data is evenly partionined but the max task duration is longer than desired for the stage, increasing the number of executors will not help and you'll need to re-partition the data. See bad partioning .","title":"Even partioning, but still slow"},{"location":"details/failed-to-read-non-parquet-file/","text":"Failed to read non-parquet file Iceberg does not perform validation on the files specified, so it will let you create a table pointing to non-supported formats, e.g. CSV data, but will fail at query time. In this case you need to use a different metastore (e.g. Hive ) If the data is stored in a supported format, it is also possible you have an invalid iceberg table, see invalid-file","title":"Failed to read non-parquet file"},{"location":"details/failed-to-read-non-parquet-file/#failed-to-read-non-parquet-file","text":"Iceberg does not perform validation on the files specified, so it will let you create a table pointing to non-supported formats, e.g. CSV data, but will fail at query time. In this case you need to use a different metastore (e.g. Hive ) If the data is stored in a supported format, it is also possible you have an invalid iceberg table, see invalid-file","title":"Failed to read non-parquet file"},{"location":"details/failure-executor-large-record/","text":"Large record failures can show up in a few different ways. For particularly large records you may find an executor out of memory exception You can get a Kyro serialization (for SQL) or Java serialization error (for RDD). Some common causes of too big records are groupByKey in RDD land, UDAFs or list aggregations in Spark SQL, highly compressed or Sparse records without a sparse seriaization. If you are uncertain of where exactely the too big record is coming from after looking at the executor logs, you can try and seperate the stage which is failing into distinct parts of the code by using persist at the DISK_ONLY level to introduce cuts into the graph. If your exception is happening with a Python UDF, it's possible that the individual records themselves might not be too large, but the batch-size used by Spark is set too high for the size of your records. You can try turning down the record size.","title":"Failure executor large record"},{"location":"details/failure-executor-out-of-memory/","text":"Executor out of memory issues can come from many different sources. To narrow down what the cause of the error there are a few important places to look: the Spark Web UI, the executor log, the driver log, and (if applicable) the cluster manager (e.g. YARN/K8s) log/UI. Container OOM If the driver log indicates Container killed by YARN for exceeding memory limits for the applicable executor, or if (on K8s) the Spark UI show's the reason for the executor loss as \"OOMKill\" / exit code 137 then it's likely your program is exceeding the amount of memory assigned to it. This doesn't normally happen with pure JVM code, but instead when calling PySpark or JNI libraries (or using off-heap storage). PySpark users are the most likely to encounter container OOMs. If you have PySpark UDF in the stage you should check out Python UDF OOM to eliminate that potential cause. Another potential issue to investigate is if your have key skew as trying to load too large a partition in Python can result in an OOM. If you are using a library, like Tensorflow, which results in","title":"Failure executor out of memory"},{"location":"details/failure-executor-out-of-memory/#container-oom","text":"If the driver log indicates Container killed by YARN for exceeding memory limits for the applicable executor, or if (on K8s) the Spark UI show's the reason for the executor loss as \"OOMKill\" / exit code 137 then it's likely your program is exceeding the amount of memory assigned to it. This doesn't normally happen with pure JVM code, but instead when calling PySpark or JNI libraries (or using off-heap storage). PySpark users are the most likely to encounter container OOMs. If you have PySpark UDF in the stage you should check out Python UDF OOM to eliminate that potential cause. Another potential issue to investigate is if your have key skew as trying to load too large a partition in Python can result in an OOM. If you are using a library, like Tensorflow, which results in","title":"Container OOM"},{"location":"details/filter_pushdown/","text":"Filter / Predicate Pushdown Processing more data than necessary will typically slow down the job. If the input table is partitioned then applying filters on the partition columns can restrict the input volume Spark needs to scan. A simple equality filter gets pushed down to the batch scan and enables Spark to only scan the files where dateint = 20211101 of a sample table partitioned on dateint and hour . select * from jlantos.sample_table where dateint = 20211101 limit 100 Examples when the filter does not get pused down The filter contains an expression If instead of a particular date we'd like to load data from the 1st of any month we might rewrite the above query such as: select * from jlantos.sample_table where dateint % 100 = 1 limit 100 The query plan shows that Spark in this case scans the whole table and filters only in a later step. Filter is dynamic via a join In a more complex job we might restrict the data based on joining to another table. If the filtering criteria is not static it won't be pushed down to the scan. So in the example below the two table scans happen independently, and min(dateint) calculated in the CTE won't have an affect on the second scan. with dates as (select min(dateint) dateint from jlantos.sample_table) select * from jlantos.sample_table st join dates d on st.dateint = d.dateint","title":"Filter / Predicate Pushdown"},{"location":"details/filter_pushdown/#filter-predicate-pushdown","text":"Processing more data than necessary will typically slow down the job. If the input table is partitioned then applying filters on the partition columns can restrict the input volume Spark needs to scan. A simple equality filter gets pushed down to the batch scan and enables Spark to only scan the files where dateint = 20211101 of a sample table partitioned on dateint and hour . select * from jlantos.sample_table where dateint = 20211101 limit 100","title":"Filter / Predicate Pushdown"},{"location":"details/filter_pushdown/#examples-when-the-filter-does-not-get-pused-down","text":"","title":"Examples when the filter does not get pused down"},{"location":"details/filter_pushdown/#the-filter-contains-an-expression","text":"If instead of a particular date we'd like to load data from the 1st of any month we might rewrite the above query such as: select * from jlantos.sample_table where dateint % 100 = 1 limit 100 The query plan shows that Spark in this case scans the whole table and filters only in a later step.","title":"The filter contains an expression"},{"location":"details/filter_pushdown/#filter-is-dynamic-via-a-join","text":"In a more complex job we might restrict the data based on joining to another table. If the filtering criteria is not static it won't be pushed down to the scan. So in the example below the two table scans happen independently, and min(dateint) calculated in the CTE won't have an affect on the second scan. with dates as (select min(dateint) dateint from jlantos.sample_table) select * from jlantos.sample_table st join dates d on st.dateint = d.dateint","title":"Filter is dynamic via a join"},{"location":"details/invalid-file/","text":"Missing Files / File Not Found / Reading past RLE/BitPacking stream Mising files are a relatively rare error in Spark. Most commonly they are caused by non-automic operations in the data writer and will go away when you re-run your query/job. On the other hand Reading past RLE/BitPacking stream or other file read errors tend to be non-transient. If the error is not transient it may mean that the metadata store (e.g. hive or iceberg) are pointing to a file that does not exist or a bad format. You can cleanup Iceberg tables using Iceberg Table Cleanup from holden's spark-misc-utils , but be careful and talk with whoever produced the table to make sure that it's ok. If you get a failed to read parquet file while you are not trying to read a parquet file, it's likely that you are using the wrong metastore .","title":"Missing Files / File Not Found / Reading past RLE/BitPacking stream"},{"location":"details/invalid-file/#missing-files-file-not-found-reading-past-rlebitpacking-stream","text":"Mising files are a relatively rare error in Spark. Most commonly they are caused by non-automic operations in the data writer and will go away when you re-run your query/job. On the other hand Reading past RLE/BitPacking stream or other file read errors tend to be non-transient. If the error is not transient it may mean that the metadata store (e.g. hive or iceberg) are pointing to a file that does not exist or a bad format. You can cleanup Iceberg tables using Iceberg Table Cleanup from holden's spark-misc-utils , but be careful and talk with whoever produced the table to make sure that it's ok. If you get a failed to read parquet file while you are not trying to read a parquet file, it's likely that you are using the wrong metastore .","title":"Missing Files / File Not Found / Reading past RLE/BitPacking stream"},{"location":"details/key-skew/","text":"Key/Partition Skew Key or partition skew is a frequent problem in Spark. Key skew can result in everything from slowly running jobs (with straglers), to failing jobs. What is data skew? Usually caused during a transformation when the data in one partition ends up being a lot more than the others, bumping up memory could resolve an OOM error but does not solve the underlying problem Processing partitions are unbalanced by a magnitude then the largest partition becomes the bottleneck How to identify skew If one task took much longer to complete than the other tasks, it's usually a sign of Skew. On the Spark UI under Summary Metrics for completed tasks if the Max duration is higher by a significant magnitude from the Median it usually represents Skew, e.g.: Things to consider Mitigating skew has a cost (e.g. repartition) hence its ignorable unless the duration or input size is significantly higher in magnitude severely impacting job time Mitigation strategies Increasing executor memory to prevent OOM exceptions -> This a short-term solution if you want to unblock yourself but does not address the underlying issue. Sometimes this is not an option when you are already running at the max memory settings allowable. Salting is a way to balance partitions by introducing a salt/dummy key for the skewed partitions. Here is a sample workbook and an example of salting in content performance show completion pipeline, where the whole salting operation is parametrized with a JOIN_BUCKETS variable which helps with maintenance of this job. Isolate the data for the skewed key, broadcast it for processing (e.g. join) and then union back the results Adaptive Query Execution is a new framework with Spark 3.0, it enables Spark to dynamically identify skew. Under the hood adaptive query execution splits (and replicates if needed) skewed (large) partitions. If you don\u2019t want to wait for 3.0, you can build the solution into the code by using the Salting/Partitioning technique listed above. Using approximate functions/ probabilistic data structure Using approximate distinct counts (Hyperloglog, bloom filter) can help get around skew if absolute precision isn't important. Approximate data structures like Tdigest can help with quantile computations. If you need exact quantiles, check out the example in High Performance Spark Certain types of aggregations and windows can result in partioning the data on a particular key.","title":"Key/Partition Skew"},{"location":"details/key-skew/#keypartition-skew","text":"Key or partition skew is a frequent problem in Spark. Key skew can result in everything from slowly running jobs (with straglers), to failing jobs. What is data skew? Usually caused during a transformation when the data in one partition ends up being a lot more than the others, bumping up memory could resolve an OOM error but does not solve the underlying problem Processing partitions are unbalanced by a magnitude then the largest partition becomes the bottleneck How to identify skew If one task took much longer to complete than the other tasks, it's usually a sign of Skew. On the Spark UI under Summary Metrics for completed tasks if the Max duration is higher by a significant magnitude from the Median it usually represents Skew, e.g.: Things to consider Mitigating skew has a cost (e.g. repartition) hence its ignorable unless the duration or input size is significantly higher in magnitude severely impacting job time Mitigation strategies Increasing executor memory to prevent OOM exceptions -> This a short-term solution if you want to unblock yourself but does not address the underlying issue. Sometimes this is not an option when you are already running at the max memory settings allowable. Salting is a way to balance partitions by introducing a salt/dummy key for the skewed partitions. Here is a sample workbook and an example of salting in content performance show completion pipeline, where the whole salting operation is parametrized with a JOIN_BUCKETS variable which helps with maintenance of this job. Isolate the data for the skewed key, broadcast it for processing (e.g. join) and then union back the results Adaptive Query Execution is a new framework with Spark 3.0, it enables Spark to dynamically identify skew. Under the hood adaptive query execution splits (and replicates if needed) skewed (large) partitions. If you don\u2019t want to wait for 3.0, you can build the solution into the code by using the Salting/Partitioning technique listed above. Using approximate functions/ probabilistic data structure Using approximate distinct counts (Hyperloglog, bloom filter) can help get around skew if absolute precision isn't important. Approximate data structures like Tdigest can help with quantile computations. If you need exact quantiles, check out the example in High Performance Spark Certain types of aggregations and windows can result in partioning the data on a particular key.","title":"Key/Partition Skew"},{"location":"details/pyudfoom/","text":"PySpark UDF / UDAF OOM Out of memory exceptions with Python user-defined-functions are especially likely as Spark doesn't do a good job of managing memory between the JVM and Python VM. Together this can result in exceeding container memory limits . Grouped Map / Co-Grouped The Grouped & Co-Grouped UDFs are especially likely to cause out-of-memory exceptions in PySpark when combined with key skew . Unlike most built in Spark aggregations, PySpark user-defined-aggregates do not support partial aggregation. This means that all of the data for a single key must fit in memory. If possible try and use an equivelent built-in aggregation, write a Scala aggregation supporting partial aggregates, or switch to an RDD and use reduceByKey . This limitation applies regardless of if you are doing Arrow or \"vanilla\" UDAFs. Arrow / Pandas / Vectorized UDFS If you are using PySpark's not-so-new Arrow based UDFS (sometimes called pandas UDFS or vectorized UDFs ), there is record batching which can cause issues. You can configure it by changing spark.sql.execution.arrow.maxRecordsPerBatch , which defaults to 10k records per batch. If your records are large this default may very well be the source of your out of memory exceptions. Note: setting spark.sql.execution.arrow.maxRecordsPerBatch too-small will result in reduce performance and reduced ability to vectorize operations over the data frames. mapInPandas / mapInArrow If you use mapInPandas or mapInArrow (proposed in 3.3+) it's important to note that Spark will serialize entire records, not just the columns needed by your UDF. If you encounter OOMs here because of record sizes, one option is to select it down to the minimal data needed to perform the UDF + a key to rejoin with the target dataset to minimize the amount of data being serialized in each record.","title":"PySpark UDF / UDAF OOM"},{"location":"details/pyudfoom/#pyspark-udf-udaf-oom","text":"Out of memory exceptions with Python user-defined-functions are especially likely as Spark doesn't do a good job of managing memory between the JVM and Python VM. Together this can result in exceeding container memory limits .","title":"PySpark UDF / UDAF OOM"},{"location":"details/pyudfoom/#grouped-map-co-grouped","text":"The Grouped & Co-Grouped UDFs are especially likely to cause out-of-memory exceptions in PySpark when combined with key skew . Unlike most built in Spark aggregations, PySpark user-defined-aggregates do not support partial aggregation. This means that all of the data for a single key must fit in memory. If possible try and use an equivelent built-in aggregation, write a Scala aggregation supporting partial aggregates, or switch to an RDD and use reduceByKey . This limitation applies regardless of if you are doing Arrow or \"vanilla\" UDAFs.","title":"Grouped Map / Co-Grouped"},{"location":"details/pyudfoom/#arrow-pandas-vectorized-udfs","text":"If you are using PySpark's not-so-new Arrow based UDFS (sometimes called pandas UDFS or vectorized UDFs ), there is record batching which can cause issues. You can configure it by changing spark.sql.execution.arrow.maxRecordsPerBatch , which defaults to 10k records per batch. If your records are large this default may very well be the source of your out of memory exceptions. Note: setting spark.sql.execution.arrow.maxRecordsPerBatch too-small will result in reduce performance and reduced ability to vectorize operations over the data frames.","title":"Arrow / Pandas / Vectorized UDFS"},{"location":"details/pyudfoom/#mapinpandas-mapinarrow","text":"If you use mapInPandas or mapInArrow (proposed in 3.3+) it's important to note that Spark will serialize entire records, not just the columns needed by your UDF. If you encounter OOMs here because of record sizes, one option is to select it down to the minimal data needed to perform the UDF + a key to rejoin with the target dataset to minimize the amount of data being serialized in each record.","title":"mapInPandas / mapInArrow"},{"location":"details/regex-tips/","text":"Regular Expression Tips Spark function regexp_extract and regexp_replace can transform data using regular expressions. The regular expression pattern follows Java regex pattern . Task Running Very Slowly Stack trace shows: java.lang.Character.codePointAt(Character.java:4884) java.util.regex.Pattern$CharProperty.match(Pattern.java:3789) java.util.regex.Pattern$Curly.match1(Pattern.java:4307) java.util.regex.Pattern$Curly.match(Pattern.java:4250) java.util.regex.Pattern$GroupHead.match(Pattern.java:4672) java.util.regex.Pattern$BmpCharProperty.match(Pattern.java:3812) java.util.regex.Pattern$Curly.match0(Pattern.java:4286) java.util.regex.Pattern$Curly.match(Pattern.java:4248) java.util.regex.Pattern$BmpCharProperty.match(Pattern.java:3812) java.util.regex.Pattern$Curly.match0(Pattern.java:4286) java.util.regex.Pattern$Curly.match(Pattern.java:4248) java.util.regex.Pattern$BmpCharProperty.match(Pattern.java:3812) java.util.regex.Pattern$Curly.match0(Pattern.java:4286) java.util.regex.Pattern$Curly.match(Pattern.java:4248) java.util.regex.Pattern$BmpCharProperty.match(Pattern.java:3812) java.util.regex.Pattern$Curly.match0(Pattern.java:4286) java.util.regex.Pattern$Curly.match(Pattern.java:4248) java.util.regex.Pattern$Start.match(Pattern.java:3475) java.util.regex.Matcher.search(Matcher.java:1248) java.util.regex.Matcher.find(Matcher.java:637) org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.RegExpExtract_2$(Unknown Source) Certain values in the dataset cause regexp_extract with a certain regex patten to run very slowly. See https://stackoverflow.com/questions/5011672/java-regular-expression-running-very-slow. Match Special Character in PySpark You will need 4 backslashes to match any special character, 2 required by Python string escaping and 2 by Java regex parsing. df = spark.sql(\"SELECT regexp_replace('{{template}}', '\\\\\\\\{\\\\\\\\{', '#')\")","title":"Regular Expression Tips"},{"location":"details/regex-tips/#regular-expression-tips","text":"Spark function regexp_extract and regexp_replace can transform data using regular expressions. The regular expression pattern follows Java regex pattern .","title":"Regular Expression Tips"},{"location":"details/regex-tips/#task-running-very-slowly","text":"Stack trace shows: java.lang.Character.codePointAt(Character.java:4884) java.util.regex.Pattern$CharProperty.match(Pattern.java:3789) java.util.regex.Pattern$Curly.match1(Pattern.java:4307) java.util.regex.Pattern$Curly.match(Pattern.java:4250) java.util.regex.Pattern$GroupHead.match(Pattern.java:4672) java.util.regex.Pattern$BmpCharProperty.match(Pattern.java:3812) java.util.regex.Pattern$Curly.match0(Pattern.java:4286) java.util.regex.Pattern$Curly.match(Pattern.java:4248) java.util.regex.Pattern$BmpCharProperty.match(Pattern.java:3812) java.util.regex.Pattern$Curly.match0(Pattern.java:4286) java.util.regex.Pattern$Curly.match(Pattern.java:4248) java.util.regex.Pattern$BmpCharProperty.match(Pattern.java:3812) java.util.regex.Pattern$Curly.match0(Pattern.java:4286) java.util.regex.Pattern$Curly.match(Pattern.java:4248) java.util.regex.Pattern$BmpCharProperty.match(Pattern.java:3812) java.util.regex.Pattern$Curly.match0(Pattern.java:4286) java.util.regex.Pattern$Curly.match(Pattern.java:4248) java.util.regex.Pattern$Start.match(Pattern.java:3475) java.util.regex.Matcher.search(Matcher.java:1248) java.util.regex.Matcher.find(Matcher.java:637) org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.RegExpExtract_2$(Unknown Source) Certain values in the dataset cause regexp_extract with a certain regex patten to run very slowly. See https://stackoverflow.com/questions/5011672/java-regular-expression-running-very-slow.","title":"Task Running Very Slowly"},{"location":"details/regex-tips/#match-special-character-in-pyspark","text":"You will need 4 backslashes to match any special character, 2 required by Python string escaping and 2 by Java regex parsing. df = spark.sql(\"SELECT regexp_replace('{{template}}', '\\\\\\\\{\\\\\\\\{', '#')\")","title":"Match Special Character in PySpark"},{"location":"details/slow-executor/","text":"Slow executor There can be many reasons executors are slow, here are a few things you can look into: - Performance distribution among tasks in the same stage: In Spark UI - Stages - Summary Metric: check if there's uneven distribution of duration / input size, if so there may be data skews or uneven partition splits. - Task size: In Spark UI - Stages - Summary Metrics, check the input/output size of tasks, usually it's reasonable with a few hundred megabytes per task. If too big, you may need to increase parallel level by configurations like spark.sql.shuffle.partitions, spark.sql.files.maxPartitionBytes - GC: Check if GC time is a small fraction of duration, if it's more than a few percents, try increase executor memory and see if any difference. If adding memory is not helping, you can now see if any optimization can be done in your code for that stage.","title":"Slow executor"},{"location":"details/slow-executor/#slow-executor","text":"There can be many reasons executors are slow, here are a few things you can look into: - Performance distribution among tasks in the same stage: In Spark UI - Stages - Summary Metric: check if there's uneven distribution of duration / input size, if so there may be data skews or uneven partition splits. - Task size: In Spark UI - Stages - Summary Metrics, check the input/output size of tasks, usually it's reasonable with a few hundred megabytes per task. If too big, you may need to increase parallel level by configurations like spark.sql.shuffle.partitions, spark.sql.files.maxPartitionBytes - GC: Check if GC time is a small fraction of duration, if it's more than a few percents, try increase executor memory and see if any difference. If adding memory is not helping, you can now see if any optimization can be done in your code for that stage.","title":"Slow executor"},{"location":"details/udfslow/","text":"Avoid UDFs for the most part User defined functions in Spark are black blox to Spark and can limit performance. When possible look for built-in alternatives. One important exception is that if you have multiple functions which must be done in Python, the advice changes a little bit. Since moving data from the JVM to Python is expensive, if you can chain together multiple Python UDFs on the same column, Spark is able to pipeline these together into a single copy to/from Python.","title":"Avoid UDFs for the most part"},{"location":"details/udfslow/#avoid-udfs-for-the-most-part","text":"User defined functions in Spark are black blox to Spark and can limit performance. When possible look for built-in alternatives. One important exception is that if you have multiple functions which must be done in Python, the advice changes a little bit. Since moving data from the JVM to Python is expensive, if you can chain together multiple Python UDFs on the same column, Spark is able to pipeline these together into a single copy to/from Python.","title":"Avoid UDFs for the most part"},{"location":"flowchart/","text":"graph TD A[Start here] B[Slow Running Job] C[I have an exception or error] A --> B A --> C click B \"slow\" \"Slow\" click C \"error\" \"Error\"","title":"Index"},{"location":"flowchart/error/","text":"graph TD C[I have an exception or error] BADAGGREGATE[Bad aggregation or window] C --> EEOOM[Spark executor ran out of memory] C --> EBOOM[Spark driver ran out of memory] C --> FILEERROR[Invalid/Missing Files] EEOOM --> KEYSKEW[Key Skew] KEYSKEW --> BADAGGREGATE ELARGERECORDS --> BADAGGREGATE EEOOM --> ELARGERECORDS[Large records] ELARGERECORDS --> ESPARSE[Sparse records] ELARGERECORDS --> EPYUDFOOM[Python UDF OOM] EEOOM --> EJSONREGEX[Json REGEX issues] EEOOM --> ECONTAINEROOM[Container out of memory] EEOOM --> ETOOBIGBROADCAST[Too big broadcast join] FILEERROR --> PARQUETBUTNOT[Failed to read non-parquet file] EBOOM --> ETOOBIGBROADCAST[Too big broadcast join] EBOOM --> ECONTAINEROOM[Container out of memory] EBOOM --> COLLECT[Using collect] ETOOBIGBROADCAST --> EFORCEDBROADCAST[Forced broadcast with disabled threshold] click EEOOM \"../../details/failure-executor-out-of-memory\" \"Executor OOM\" click ESPARSE \"../../details/sparse-records\" \"Sparse records\" click COLLECT \"../../details/collect\" \"Collect and friends\" click ETOOBIGBROADCAST \"../../details/big-broadcast-join\" \"Broadcast Joins\" click FILEERROR \"../../details/invalid-file\" \"Invalid or missing files\" click PARQUETBUTNOT \"../../details/failed-to-read-non-parquet-file\" \"Failed to read non parquet file\" click EPYUDFOOM \"../../details/pyudfoom\" \"Udf OOM\" click EFORCEDBROADCAST \"../../details/broadcast-with-disable\" \"Forced broadcast with disabled\" KEYSKEW[Key skew] BADAGGREGATE[Bad aggregation or window] KEYSKEW --> BADAGGREGATE click KEYSKEW \"../../details/key-skew\" \"Key skew\" click BADAGREGATE \"../../details/bad-aggregate\" \"Bad aggregation\"","title":"Error"},{"location":"flowchart/shared/","text":"graph TD KEYSKEW[Key skew] BADAGGREGATE[Bad aggregation or window] KEYSKEW --> BADAGGREGATE click KEYSKEW \"../../details/key-skew\" \"Key skew\" click BADAGREGATE \"../../details/bad-aggregate\" \"Bad aggregation\" OHNOES[Contact support]","title":"Shared"},{"location":"flowchart/slow/","text":"graph TD B[Slow] B --> SLOWSTAGE[Slow stage] SLOWSTAGE --> PARTIONING[Partioning] SLOWSTAGE --> SLOWEXEC[Slow executor] SLOWSTAGE --> UDFSLOWNESS[udfslowness] UDFSLOWNESS --> PAGGS[Partial aggregates] PARTIONING --> GOODPART_SLOW[Even partioning] PARTIONING --> BADPART[Uneven partioning] BADPART --> KEYSKEW B --> TOOMUCHDATA[Reading more data than needed] TOOMUCHDATA --> FILTERNOTPUSHED[Filter not pushed down] TOOMUCHDATA --> AGGNOTPUSHED[Aggregation not pushed down] TOOMUCHDATA --> STORAGE_PARTITIONING[Bad storage partioning] click BADPART \"../../details/bad_partioning\" \"Slow stage with uneven partioning\" click GOODPART_SLOW \"../../details/even_partioning_still_slow\" \"Slow stage with even partioning\" click UDFSLOWNESS \"../../udfslow\" click PAGGS \"../../partial_aggregates\" click FILTERNOTPUSHED \"../../details/filter_pushdown\" KEYSKEW[Key skew] BADAGGREGATE[Bad aggregation or window] KEYSKEW --> BADAGGREGATE click KEYSKEW \"../../details/key-skew\" \"Key skew\" click BADAGREGATE \"../../details/bad-aggregate\" \"Bad aggregation\"","title":"Slow"}]}